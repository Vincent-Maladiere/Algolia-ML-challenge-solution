{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737ce613",
   "metadata": {},
   "source": [
    "# II. Baseline #1\n",
    "\n",
    "**Questions and users' answers featurization**\n",
    "\n",
    "We write $f(q) \\in \\mathbb{R}^{768}$ the embeddings of a question $q$, assuming token embedding size of 768, and\n",
    "$g(u) \\in \\mathbb{R}^{768}$ the embeddings of a user $u$.\n",
    "\n",
    "For these embeddings, we will consider only the `title` of questions and the `text` answers from users.\n",
    "- Question embeddings are defined by the average embedding on each word of the title.\n",
    "- User embeddings are defined by the average embedding on each answer by this user (one answer embedding being the average of its word embeddings).\n",
    "\n",
    "We will leverage a pre-trained BERT model from HuggingFace to create these embeddings.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Model**\n",
    "\n",
    "For a given new question, our baseline algorithm consists in returning the top-20 dot product between the question embedding and users embeddings:\n",
    "\n",
    "$$max f(q)^T . g(u)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Pros of this model\n",
    "- a sensible baseline\n",
    "- no training needed and fast to try out\n",
    "\n",
    "\n",
    "Cons\n",
    "- not flexible\n",
    "- doesn't leverage explicit ratings\n",
    "- not sure NLP semantics will be relevant enough to suggest the most likely users to answer a question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36cc24",
   "metadata": {},
   "source": [
    "## Loading data and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fae52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentmaladiere/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from src.data import load_data, split_questions, save_results_csv, DATA_PATH\n",
    "from src.embedder import BertEmbedder\n",
    "from src.score import precision_k, recall_k\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ca504",
   "metadata": {},
   "source": [
    "Set `USE_PRECOMPUTE` to `True` once you have computed embeddings. This will result in a significant speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8b9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRECOMPUTE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88009461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_answers.shape: (95709, 6)\n",
      "df_questions.shape: (100000, 6)\n",
      "df_users.shape: (138698, 12)\n"
     ]
    }
   ],
   "source": [
    "df_answers, df_questions, df_users = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c5c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_train.shape: (65036, 8)\n",
      "Q_val.shape: (5000, 8)\n",
      "Q_test.shape: (1000, 8)\n"
     ]
    }
   ],
   "source": [
    "Q_train, Q_val, Q_test = split_questions(df_questions, df_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94fde60",
   "metadata": {},
   "source": [
    "A quick demo of our embeddings: here, 2 sentences from \"cosmos\" in Wikipedia are compared to the introduction of \"cheeseburger\", also in wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ff9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_1 x text_1: tensor([[6593.2627]])\n",
      "text_1 x text_2: tensor([[6189.0732]])\n",
      "text_2 x text_3: tensor([[5542.9907]])\n",
      "text_1 x text_3: tensor([[5702.2930]])\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"The cosmos, and our understanding of the reasons for its existence and significance, are studied in cosmology â€“ a broad discipline covering scientific, religious or philosophical aspects of the cosmos and its nature.\"\n",
    "text_2 = \"Religious and philosophical approaches may include the cosmos among spiritual entities or other matters deemed to exist outside our physical universe.\"\n",
    "text_3 = \"The cheese is usually added to the cooking hamburger patty shortly before serving, which allows the cheese to melt.\"\n",
    "\n",
    "embedder = BertEmbedder()\n",
    "out_1 = embedder.get_embeddings(text_1)\n",
    "out_2 = embedder.get_embeddings(text_2)\n",
    "out_3 = embedder.get_embeddings(text_3)\n",
    "\n",
    "print(f\"text_1 x text_1: {out_1[None] @ out_1[None].T}\")\n",
    "print(f\"text_1 x text_2: {out_1[None] @ out_2[None].T}\")\n",
    "print(f\"text_2 x text_3: {out_2[None] @ out_3[None].T}\")\n",
    "print(f\"text_1 x text_3: {out_1[None] @ out_3[None].T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dd248",
   "metadata": {},
   "source": [
    "The product of the first vector with itself is obviously the highest. We then find the two cosmos sentences on the second rank, as we expected. However all scores seems rather close to each other, it migth indicate that this scoring method is quite noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2dd2",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**Clean users' answers**\n",
    "\n",
    "Users answers are formatted in a raw html format, so we need to parse the relevant information that the BERT transformer will leverage. Besides, users often include code snippets in their answer, and we need to remove those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e192e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def clean_answer(raw_html):\n",
    "    raw_html = clean_code_snippets(raw_html)\n",
    "    txt = clean_html(raw_html)\n",
    "    txt = clean_punctuation(txt)\n",
    "    return txt\n",
    "\n",
    "def clean_code_snippets(raw_html):\n",
    "    # select content outside of <code> </code>\n",
    "    chunks = raw_html.split(\"<code>\")\n",
    "    clean_txt = \"\"\n",
    "    for chunk in chunks:\n",
    "        sub_chunks = chunk.split(\"</code>\")\n",
    "        if len(sub_chunks) > 1:\n",
    "            text = sub_chunks[1]\n",
    "        else:\n",
    "            text = sub_chunks[0]\n",
    "        clean_txt = \" \".join([clean_txt, text])\n",
    "    return clean_txt\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def clean_punctuation(txt):\n",
    "    return txt.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3fe99",
   "metadata": {},
   "source": [
    "Example of answer including a code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e611527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original answer is: \n",
      "'<p>You should try this plugin:</p>\\n<p><a href=\"https://pub.dev/packages/custom_splash\" rel=\"nofollow noreferrer\">customSplash</a></p>\\n<p>And there are still more on pub.dev.</p>\\n<p>The code for custom splash:</p>\\n<pre><code>runApp(MaterialApp(\\n    home: CustomSplash(\\n        imagePath: \\'assets/flutter_icon.png\\',\\n        backGroundColor: Colors.deepOrange,\\n        animationEffect: \\'zoom-in\\',\\n        logoSize: 200,\\n        home: MyApp(),\\n        customFunction: duringSplash,\\n        duration: 2500,\\n        type: CustomSplashType.StaticDuration,\\n        outputAndHome: op,\\n    ),\\n));\\n</code></pre>'\n",
      "==================\n",
      "cleaned answer is: \n",
      "'You should try this plugin: customSplash And there are still more on pub.dev. The code for custom splash:'\n"
     ]
    }
   ],
   "source": [
    "answer_idx = 5000\n",
    "txt = df_answers.iloc[answer_idx].text\n",
    "print(f\"original answer is: \\n{repr(txt)}\")\n",
    "print(\"=\"*18)\n",
    "txt = clean_answer(txt)\n",
    "print(f\"cleaned answer is: \\n{repr(txt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28053b5",
   "metadata": {},
   "source": [
    "**Let's embed top users answers**\n",
    "\n",
    "First, we need to define a subset of users that has answered to, say, at least 3 questions. We will create embedding indexes for each of these users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e3f3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users_embeddings(file_name):\n",
    "    file_path = os.path.join(DATA_PATH, \"intermediary\", file_name)\n",
    "    return pd.read_pickle(file_path)\n",
    "    \n",
    "\n",
    "def compute_users_embedding(df_answers, user_ids, embedder):\n",
    "    max_text_size = 500\n",
    "    user_2_embeddings = {}\n",
    "    n_skip_user = 0\n",
    "    for user_id in tqdm(user_ids):\n",
    "        texts = df_answers.loc[df_answers.user_id == user_id].text.values\n",
    "        user_embeddings = []\n",
    "        for text in texts:\n",
    "            text = clean_answer(text)[:max_text_size]\n",
    "            if text:\n",
    "                text_embeddings = embedder.get_embeddings(text)\n",
    "                if not text_embeddings is None:\n",
    "                    user_embeddings.append(text_embeddings)\n",
    "        if user_embeddings:\n",
    "            user_embeddings = torch.stack(user_embeddings).mean(dim=0)\n",
    "            user_2_embeddings[user_id] = user_embeddings\n",
    "        else:\n",
    "            n_skip_user += 1\n",
    "\n",
    "    print(f\"{n_skip_user} were skipped\")\n",
    "    \n",
    "    save_pickle(\"user_embeddings.pkl\", user_2_embeddings)\n",
    "    \n",
    "    return user_2_embeddings\n",
    "\n",
    "\n",
    "def filter_top_users(df_users, df_answers, k_answers=3):\n",
    "    group_user = df_answers.groupby(\"user_id\").answer_id.count() \\\n",
    "                           .reset_index().rename(columns={\"answer_id\": \"total\"})\n",
    "    df_merge = df_users.merge(group_user, left_on=\"id\", right_on=\"user_id\", how=\"inner\")\n",
    "    df_merge = df_merge.loc[df_merge.total >= k_answers]\n",
    "    \n",
    "    return df_merge.user_id.values\n",
    "\n",
    "\n",
    "def save_pickle(file_name, content):\n",
    "    file_path = os.path.join(DATA_PATH, \"intermediary\", file_name)\n",
    "    pickle.dump(content, open(file_path, \"wb+\"))\n",
    "    print(f\"{file_path} written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0519640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_user_ids = filter_top_users(df_users, df_answers, k_answers=1)\n",
    "len(top_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154c17ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44880"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_PRECOMPUTE:\n",
    "    user_2_embeddings = load_users_embeddings(\"user_embeddings.pkl\")\n",
    "else:\n",
    "    user_2_embeddings = compute_users_embedding(df_answers, top_user_ids, embedder)\n",
    "len(user_2_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244884f",
   "metadata": {},
   "source": [
    "**Let's now define our model and encode some test questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8f1c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_top_users_precompute(Q_test, user_2_embeddings, df_q_embedding):\n",
    "    user_ids, U = get_u_matrix(user_2_embeddings)\n",
    "    results = []\n",
    "    for question_id in tqdm(Q_test.question_id.values):\n",
    "        if question_id in df_q_embedding.index:\n",
    "            q_embedding = df_q_embedding.loc[question_id].embeddings\n",
    "            top_user_ids = get_top_k_users_precompute(q_embedding, U, user_ids)\n",
    "            results.append(np.hstack([question_id, top_user_ids]))\n",
    "    save_results_csv(\"baseline_1_results.csv\", results)\n",
    "\n",
    "    \n",
    "def get_top_k_users_precompute(q, U, user_ids, k=20):\n",
    "    r = U @ q[None].T\n",
    "    top_idxs = torch.argsort(r, dim=0, descending=True)[:k].numpy().reshape(1, -1)\n",
    "    top_user_ids = user_ids[top_idxs]\n",
    "    return top_user_ids.ravel()\n",
    "    \n",
    "\n",
    "def compute_questions_top_users(Q_test, user_2_embeddings, embedder):\n",
    "    user_ids, U = get_u_matrix(user_2_embeddings)\n",
    "    cols = Q_test.columns\n",
    "    col2idx = dict(zip(cols, range(len(cols))))\n",
    "    results = []\n",
    "    for row in tqdm(Q_test.values):\n",
    "        title = row[col2idx[\"title\"]]\n",
    "        question_id = row[col2idx[\"question_id\"]]\n",
    "        top_user_ids = get_top_k_users(title, U, user_ids, embedder)\n",
    "        results.append(np.hstack([question_id, top_user_ids]))\n",
    "    save_results_csv(\"baseline_results.csv\", results)\n",
    "\n",
    "    \n",
    "def get_top_k_users(title, U, user_ids, embedder, k=20):\n",
    "    q = embedder.get_embeddings(title)\n",
    "    r = U @ q[None].T\n",
    "    top_idxs = torch.argsort(r, dim=0, descending=True)[:k].numpy().reshape(1, -1)\n",
    "    top_user_ids = user_ids[top_idxs]\n",
    "    return top_user_ids.ravel()\n",
    "\n",
    "\n",
    "def get_u_matrix(user_2_embeddings):\n",
    "    user_ids = np.array(list(user_2_embeddings.keys()))\n",
    "    U = torch.stack(list(user_2_embeddings.values()))\n",
    "    return user_ids, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "711696bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47b9779967b4a188901153f1abb4a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/results/baseline_results.csv written\n"
     ]
    }
   ],
   "source": [
    "if USE_PRECOMPUTE:\n",
    "    file_path = os.path.join(DATA_PATH, \"intermediary\", \"q_embeddings.pkl\")\n",
    "    df_q_embeddings = pd.read_pickle(file_path).set_index(\"question_id\")\n",
    "    get_questions_top_users_precompute(Q_val, user_2_embeddings, df_q_embeddings)\n",
    "else:\n",
    "    get_questions_top_users(Q_val, user_2_embeddings, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c951bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1_id</th>\n",
       "      <th>user2_id</th>\n",
       "      <th>user3_id</th>\n",
       "      <th>user4_id</th>\n",
       "      <th>user5_id</th>\n",
       "      <th>user6_id</th>\n",
       "      <th>user7_id</th>\n",
       "      <th>user8_id</th>\n",
       "      <th>user9_id</th>\n",
       "      <th>user10_id</th>\n",
       "      <th>user11_id</th>\n",
       "      <th>user12_id</th>\n",
       "      <th>user13_id</th>\n",
       "      <th>user14_id</th>\n",
       "      <th>user15_id</th>\n",
       "      <th>user16_id</th>\n",
       "      <th>user17_id</th>\n",
       "      <th>user18_id</th>\n",
       "      <th>user19_id</th>\n",
       "      <th>user20_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59758505</th>\n",
       "      <td>12073046</td>\n",
       "      <td>12000088</td>\n",
       "      <td>13619217</td>\n",
       "      <td>9701547</td>\n",
       "      <td>1394729</td>\n",
       "      <td>7202022</td>\n",
       "      <td>12423397</td>\n",
       "      <td>8854888</td>\n",
       "      <td>6387370</td>\n",
       "      <td>13396042</td>\n",
       "      <td>12788763</td>\n",
       "      <td>3384658</td>\n",
       "      <td>7394739</td>\n",
       "      <td>12992796</td>\n",
       "      <td>3618301</td>\n",
       "      <td>9119669</td>\n",
       "      <td>3641900</td>\n",
       "      <td>13949605</td>\n",
       "      <td>12531994</td>\n",
       "      <td>1404347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63187776</th>\n",
       "      <td>9701547</td>\n",
       "      <td>12423397</td>\n",
       "      <td>7394739</td>\n",
       "      <td>8804776</td>\n",
       "      <td>9982227</td>\n",
       "      <td>1394729</td>\n",
       "      <td>6387370</td>\n",
       "      <td>13619217</td>\n",
       "      <td>12073046</td>\n",
       "      <td>4626254</td>\n",
       "      <td>10152069</td>\n",
       "      <td>6587870</td>\n",
       "      <td>11480455</td>\n",
       "      <td>3802507</td>\n",
       "      <td>13076470</td>\n",
       "      <td>5588286</td>\n",
       "      <td>14446599</td>\n",
       "      <td>12531994</td>\n",
       "      <td>13983739</td>\n",
       "      <td>3618301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61603799</th>\n",
       "      <td>9701547</td>\n",
       "      <td>7248771</td>\n",
       "      <td>12073046</td>\n",
       "      <td>70918</td>\n",
       "      <td>7357999</td>\n",
       "      <td>2859017</td>\n",
       "      <td>5787139</td>\n",
       "      <td>1394729</td>\n",
       "      <td>12446721</td>\n",
       "      <td>12423397</td>\n",
       "      <td>7394739</td>\n",
       "      <td>3464777</td>\n",
       "      <td>13619217</td>\n",
       "      <td>12531994</td>\n",
       "      <td>12406503</td>\n",
       "      <td>11462013</td>\n",
       "      <td>13949605</td>\n",
       "      <td>9982227</td>\n",
       "      <td>976470</td>\n",
       "      <td>1404347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user1_id  user2_id  user3_id  user4_id  user5_id  user6_id  \\\n",
       "question_id                                                               \n",
       "59758505     12073046  12000088  13619217   9701547   1394729   7202022   \n",
       "63187776      9701547  12423397   7394739   8804776   9982227   1394729   \n",
       "61603799      9701547   7248771  12073046     70918   7357999   2859017   \n",
       "\n",
       "             user7_id  user8_id  user9_id  user10_id  user11_id  user12_id  \\\n",
       "question_id                                                                  \n",
       "59758505     12423397   8854888   6387370   13396042   12788763    3384658   \n",
       "63187776      6387370  13619217  12073046    4626254   10152069    6587870   \n",
       "61603799      5787139   1394729  12446721   12423397    7394739    3464777   \n",
       "\n",
       "             user13_id  user14_id  user15_id  user16_id  user17_id  user18_id  \\\n",
       "question_id                                                                     \n",
       "59758505       7394739   12992796    3618301    9119669    3641900   13949605   \n",
       "63187776      11480455    3802507   13076470    5588286   14446599   12531994   \n",
       "61603799      13619217   12531994   12406503   11462013   13949605    9982227   \n",
       "\n",
       "             user19_id  user20_id  \n",
       "question_id                        \n",
       "59758505      12531994    1404347  \n",
       "63187776      13983739    3618301  \n",
       "61603799        976470    1404347  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(DATA_PATH, \"results\", \"baseline_1_results.csv\")\n",
    "df_results = pd.read_csv(file_path, index_col=\"question_id\")\n",
    "df_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f8d4a",
   "metadata": {},
   "source": [
    "**Get our precision score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d25595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @20: 7.100831811726518e-05\n",
      "Recall @20: 0.0011158449989855954\n"
     ]
    }
   ],
   "source": [
    "R = df_results.values\n",
    "# order the actual answers base on the prediction\n",
    "df_actual = pd.DataFrame(df_answers.groupby(\"question_id\").user_id.apply(list))\n",
    "A = list(df_actual.loc[df_results.index].values[:, 0])\n",
    "\n",
    "print(f\"Precision @20: {precision_k(Y=A, Y_pred=R)}\")\n",
    "print(f\"Recall @20: {recall_k(Y=A, Y_pred=R)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac2e5a",
   "metadata": {},
   "source": [
    "Compare it to dummy prediction where we simply select the top 20 users with the most answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6feaf427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @20: 0.0032\n",
      "Recall @20: 0.0478\n"
     ]
    }
   ],
   "source": [
    "top_20_users = df_answers.user_id.value_counts().head(20)\n",
    "R_dummy = [top_20_users.index] * len(A)\n",
    "\n",
    "print(f\"Precision @20: {precision_k(Y=A, Y_pred=R_dummy):.4f}\")\n",
    "print(f\"Recall @20: {recall_k(Y=A, Y_pred=R_dummy):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc0b6c",
   "metadata": {},
   "source": [
    "Our first baseline performs very poorly. Let's propose a new baseline method, were we will fetch users from similar questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854c812",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccc011",
   "metadata": {},
   "source": [
    "Had this first baseline showed some sign of success, we could have tried to learn users embeddings instead of using NLP. \n",
    "\n",
    "### V2 approach: learning user embeddings\n",
    "\n",
    "For this new approach, we keep the same embedding strategy for questions, but we want to learn an embedding for users. The algorithm don't change, we still keep the top-20 dot product between the users and questions.\n",
    "\n",
    "$$max f(q)^T . g(u)$$\n",
    "\n",
    "**Learning algorithm**\n",
    "- Let's note $x_u \\in \\mathbb{R}^{728}$ a user embeddings to learn.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "As a cons, we need to keep in mind that learning embeddings for users with few answers is prone to overfitting, so we may need to limit our user list. Here is some pseudo-code for this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e01af66",
   "metadata": {},
   "source": [
    "```python\n",
    "# neural net trained by pytorch\n",
    "embed_user = torch.nn.Embedding(num_classes=len(dataset.users.unique()), embed_size=768)\n",
    "\n",
    "opt = torch.optim.SGD(embed_user.parameters(), weight_decay=some_chosen_regularization_value)\n",
    "\n",
    "# everything below is batched\n",
    "# the random user may or may have not answered the question sampled with them\n",
    "for question_id, random_user in zip(dataset.questions, dataset.users):\n",
    "    f_q = embed_nlp_fixed_params(question.title)\n",
    "    \n",
    "    # users who actually answered the questions should map to 1.0 dot product\n",
    "    for user_who_answered in question.users:\n",
    "        x_u = embed_user(user_who_answered.user_id)\n",
    "        loss = (1.0 - x_u.dot(f_q)) ** 2\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    # random user should map to 0.0 dot product for this question\n",
    "    x_u = embed_user\n",
    "    loss = (0.0 - x_u.dot(f_q)) ** 2\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    " ```\n",
    " \n",
    " At eval time nothing changes, except we have different embeddings. We still take the top 20 users with the largest dot product:\n",
    "\n",
    "$$\\max x_u^T.f(q)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc5dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
